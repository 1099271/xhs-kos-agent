"""
AIå¢å¼ºç‰ˆMulti-Agentå·¥ä½œæµæµ‹è¯•è„šæœ¬
æµ‹è¯•é›†æˆLLMæ¨¡å‹çš„æ™ºèƒ½åŒ–å¤šAgentåä½œç³»ç»Ÿ
"""

import asyncio
import sys
import os
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from app.agents.enhanced_multi_agent_workflow import (
    EnhancedMultiAgentWorkflow, 
    test_enhanced_multi_agent_workflow
)
from app.agents.llm_manager import ModelProvider
from app.utils.logger import app_logger as logger


async def test_enhanced_workflow_basic():
    """æµ‹è¯•AIå¢å¼ºç‰ˆå·¥ä½œæµåŸºç¡€åŠŸèƒ½"""
    
    logger.info("=== æµ‹è¯•AIå¢å¼ºç‰ˆå·¥ä½œæµåŸºç¡€åŠŸèƒ½ ===")
    
    try:
        # åˆ›å»ºå¢å¼ºç‰ˆå·¥ä½œæµå®ä¾‹
        workflow = EnhancedMultiAgentWorkflow()
        logger.info("âœ… AIå¢å¼ºç‰ˆå·¥ä½œæµå®ä¾‹åˆ›å»ºæˆåŠŸ")
        
        # æ£€æŸ¥å›¾æ˜¯å¦æ­£ç¡®æ„å»º
        assert workflow.graph is not None, "å·¥ä½œæµå›¾æœªæ­£ç¡®æ„å»º"
        logger.info("âœ… å·¥ä½œæµå›¾æ„å»ºæˆåŠŸ")
        
        # æ£€æŸ¥LLMç»„ä»¶æ˜¯å¦åˆå§‹åŒ–
        assert workflow.llm_manager is not None, "LLMç®¡ç†å™¨æœªåˆå§‹åŒ–"
        assert workflow.user_analyst_llm is not None, "ç”¨æˆ·åˆ†æLLMè°ƒç”¨å™¨æœªåˆå§‹åŒ–"
        logger.info("âœ… LLMç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
        
        # æ£€æŸ¥å¯ç”¨æ¨¡å‹
        available_providers = workflow.llm_manager.get_available_providers()
        logger.info(f"å¯ç”¨LLMæ¨¡å‹: {[p.value for p in available_providers]}")
        
        assert len(available_providers) > 0, "è‡³å°‘åº”è¯¥æœ‰ä¸€ä¸ªå¯ç”¨çš„LLMæ¨¡å‹"
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ åŸºç¡€åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_enhanced_workflow_execution():
    """æµ‹è¯•AIå¢å¼ºç‰ˆå·¥ä½œæµå®Œæ•´æ‰§è¡Œ"""
    
    logger.info("=== æµ‹è¯•AIå¢å¼ºç‰ˆå·¥ä½œæµå®Œæ•´æ‰§è¡Œ ===")
    
    try:
        # ä½¿ç”¨OpenRouterä½œä¸ºé¦–é€‰æ¨¡å‹ï¼ˆé€šå¸¸æ¯”è¾ƒç¨³å®šï¼‰
        workflow = EnhancedMultiAgentWorkflow(
            preferred_model_provider=ModelProvider.OPENROUTER
        )
        
        start_time = datetime.now()
        
        # æ‰§è¡Œå¢å¼ºç‰ˆå·¥ä½œæµ
        result = await workflow.execute_enhanced_workflow({
            "task": "AIå¢å¼ºç‰ˆUGCå¹³å°é«˜ä»·å€¼ç”¨æˆ·åˆ†æä¸æ™ºèƒ½å†…å®¹ç­–ç•¥",
            "parameters": {
                "target_user_count": 20,
                "content_themes": ["AIä¸ªæ€§åŒ–æ¨è", "æ™ºèƒ½æ•°æ®æ´å¯Ÿ", "ç”¨æˆ·ä½“éªŒä¼˜åŒ–"],
                "ai_enhancement": True,
                "preferred_model": "openrouter"
            }
        })
        
        execution_time = (datetime.now() - start_time).total_seconds()
        
        # éªŒè¯ç»“æœ
        assert result.get("success"), "å·¥ä½œæµæ‰§è¡Œåº”è¯¥æˆåŠŸ"
        assert result.get("enhanced_features"), "åº”è¯¥å¯ç”¨å¢å¼ºåŠŸèƒ½"
        
        logger.info(f"âœ… å·¥ä½œæµæ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        
        # æ£€æŸ¥AIå¢å¼ºåŠŸèƒ½
        llm_insights = result.get("llm_insights", {})
        logger.info(f"ç”Ÿæˆçš„AIæ´å¯Ÿæ•°: {len(llm_insights)}")
        
        agent_results = result.get("agent_results", [])
        enhanced_agents = [r for r in agent_results if hasattr(r, 'llm_analysis') and r.llm_analysis]
        logger.info(f"åŒ…å«LLMåˆ†æçš„Agentæ•°: {len(enhanced_agents)}")
        
        # è¾“å‡ºè¯¦ç»†ç»“æœ
        print(f"\n{'='*60}")
        print(f"AIå¢å¼ºç‰ˆå·¥ä½œæµæ‰§è¡Œç»“æœ:")
        print(f"{'='*60}")
        print(f"æ‰§è¡ŒæˆåŠŸ: {result.get('success')}")
        print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        print(f"AIå¢å¼º: {result.get('enhanced_features')}")
        print(f"æ‰§è¡Œæ‘˜è¦: {result.get('execution_summary', 'æ— æ‘˜è¦')}")
        print(f"AIå¢å¼ºæ‘˜è¦: {result.get('ai_enhancement_summary', 'æ— AIæ‘˜è¦')}")
        
        # æ˜¾ç¤ºAgentæ‰§è¡Œç»“æœ
        if agent_results:
            print(f"\nå„Agentæ‰§è¡Œè¯¦æƒ…:")
            for agent_result in agent_results:
                status = "âœ…" if agent_result.success else "âŒ"
                print(f"{status} {agent_result.agent_name}:")
                print(f"   æ‰§è¡Œæ—¶é—´: {agent_result.execution_time:.2f}ç§’")
                print(f"   ç»“æœ: {agent_result.message}")
                if hasattr(agent_result, 'llm_analysis') and agent_result.llm_analysis:
                    print(f"   ğŸ§  AIæ´å¯Ÿ: {agent_result.llm_analysis[:150]}...")
        
        # æ˜¾ç¤ºLLMæ´å¯Ÿ
        if llm_insights:
            print(f"\nğŸ” LLMæ´å¯Ÿæ±‡æ€»:")
            for phase, insight in llm_insights.items():
                print(f"- {phase}: {insight[:200]}...")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ å®Œæ•´æ‰§è¡Œæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_different_model_providers():
    """æµ‹è¯•ä¸åŒæ¨¡å‹æä¾›å•†çš„å…¼å®¹æ€§"""
    
    logger.info("=== æµ‹è¯•ä¸åŒæ¨¡å‹æä¾›å•†å…¼å®¹æ€§ ===")
    
    # å¯ç”¨çš„æ¨¡å‹æä¾›å•†
    providers_to_test = [
        ModelProvider.OPENROUTER,
        ModelProvider.QWEN,
        ModelProvider.DEEPSEEK,
        # ModelProvider.ANTHROPIC,  # å¯èƒ½è¿æ¥ä¸ç¨³å®šï¼Œå…ˆè·³è¿‡
    ]
    
    results = {}
    
    for provider in providers_to_test:
        try:
            logger.info(f"æµ‹è¯•æ¨¡å‹æä¾›å•†: {provider.value}")
            
            workflow = EnhancedMultiAgentWorkflow(preferred_model_provider=provider)
            
            # ç®€åŒ–çš„æµ‹è¯•ä»»åŠ¡
            start_time = datetime.now()
            result = await workflow.execute_enhanced_workflow({
                "task": f"ä½¿ç”¨{provider.value}æ¨¡å‹çš„ç®€åŒ–æµ‹è¯•",
                "parameters": {"test_mode": True}
            })
            execution_time = (datetime.now() - start_time).total_seconds()
            
            results[provider.value] = {
                "success": result.get("success", False),
                "execution_time": execution_time,
                "ai_insights_count": len(result.get("llm_insights", {})),
                "error": result.get("error") if not result.get("success") else None
            }
            
            logger.info(f"âœ… {provider.value} æµ‹è¯•å®Œæˆ: {execution_time:.2f}ç§’")
            
        except Exception as e:
            logger.warning(f"âš ï¸ {provider.value} æµ‹è¯•å¤±è´¥: {e}")
            results[provider.value] = {
                "success": False,
                "execution_time": 0,
                "ai_insights_count": 0,
                "error": str(e)
            }
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print(f"\nğŸ“Š æ¨¡å‹æä¾›å•†å…¼å®¹æ€§æµ‹è¯•ç»“æœ:")
    for provider, metrics in results.items():
        status = "âœ…" if metrics["success"] else "âŒ"
        print(f"{status} {provider}: "
              f"è€—æ—¶{metrics['execution_time']:.2f}s, "
              f"AIæ´å¯Ÿ{metrics['ai_insights_count']}ä¸ª")
        if metrics.get("error"):
            print(f"   é”™è¯¯: {metrics['error']}")
    
    # ç»Ÿè®¡æˆåŠŸç‡
    successful_providers = [p for p, m in results.items() if m["success"]]
    success_rate = len(successful_providers) / len(results) if results else 0
    
    logger.info(f"æ¨¡å‹æä¾›å•†å…¼å®¹æ€§: {len(successful_providers)}/{len(results)} "
               f"({success_rate*100:.1f}%) æˆåŠŸ")
    
    return success_rate > 0.5  # è‡³å°‘50%æˆåŠŸç‡


async def test_llm_enhancement_quality():
    """æµ‹è¯•LLMå¢å¼ºåŠŸèƒ½çš„è´¨é‡"""
    
    logger.info("=== æµ‹è¯•LLMå¢å¼ºåŠŸèƒ½è´¨é‡ ===")
    
    try:
        workflow = EnhancedMultiAgentWorkflow()
        
        # æ‰§è¡Œæµ‹è¯•
        result = await workflow.execute_enhanced_workflow({
            "task": "LLMå¢å¼ºåŠŸèƒ½è´¨é‡æµ‹è¯•",
            "parameters": {
                "test_quality": True,
                "expected_insights": ["ç”¨æˆ·åˆ†æ", "ç­–ç•¥åˆ¶å®š", "å†…å®¹ç”Ÿæˆ", "åè°ƒä¼˜åŒ–"]
            }
        })
        
        # è´¨é‡æ£€æŸ¥
        quality_metrics = {
            "workflow_success": result.get("success", False),
            "ai_insights_generated": len(result.get("llm_insights", {})),
            "agents_with_llm_analysis": 0,
            "content_pieces_generated": 0,
            "strategy_segments_created": 0
        }
        
        # æ£€æŸ¥Agent LLMåˆ†æ
        agent_results = result.get("agent_results", [])
        for agent_result in agent_results:
            if hasattr(agent_result, 'llm_analysis') and agent_result.llm_analysis:
                quality_metrics["agents_with_llm_analysis"] += 1
        
        # æ£€æŸ¥å†…å®¹ç”Ÿæˆè´¨é‡
        generated_content = result.get("generated_content", {})
        if generated_content:
            quality_metrics["content_pieces_generated"] = len(
                generated_content.get("content_pieces", [])
            )
        
        # æ£€æŸ¥ç­–ç•¥è´¨é‡
        content_strategy = result.get("content_strategy", {})
        if content_strategy:
            quality_metrics["strategy_segments_created"] = len(
                content_strategy.get("target_segments", [])
            )
        
        # è¾“å‡ºè´¨é‡æŠ¥å‘Š
        print(f"\nğŸ“Š LLMå¢å¼ºåŠŸèƒ½è´¨é‡æŠ¥å‘Š:")
        print(f"å·¥ä½œæµæˆåŠŸ: {quality_metrics['workflow_success']}")
        print(f"AIæ´å¯Ÿç”Ÿæˆæ•°: {quality_metrics['ai_insights_generated']}")
        print(f"åŒ…å«LLMåˆ†æçš„Agentæ•°: {quality_metrics['agents_with_llm_analysis']}")
        print(f"ç”Ÿæˆå†…å®¹ç‰‡æ®µæ•°: {quality_metrics['content_pieces_generated']}")
        print(f"ç­–ç•¥ç»†åˆ†æ•°: {quality_metrics['strategy_segments_created']}")
        
        # è®¡ç®—è´¨é‡åˆ†æ•°
        max_possible_score = 10  # å‡è®¾çš„æœ€é«˜åˆ†
        actual_score = (
            (3 if quality_metrics["workflow_success"] else 0) +
            min(quality_metrics["ai_insights_generated"], 3) +
            min(quality_metrics["agents_with_llm_analysis"], 2) +
            min(quality_metrics["content_pieces_generated"] / 2, 2)
        )
        
        quality_score = actual_score / max_possible_score
        logger.info(f"LLMå¢å¼ºåŠŸèƒ½è´¨é‡åˆ†æ•°: {quality_score*100:.1f}%")
        
        return quality_score > 0.6  # è‡³å°‘60%è´¨é‡åˆ†æ•°
        
    except Exception as e:
        logger.error(f"âŒ LLMå¢å¼ºåŠŸèƒ½è´¨é‡æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_error_handling_and_fallback():
    """æµ‹è¯•é”™è¯¯å¤„ç†å’Œå¤‡é€‰æœºåˆ¶"""
    
    logger.info("=== æµ‹è¯•é”™è¯¯å¤„ç†å’Œå¤‡é€‰æœºåˆ¶ ===")
    
    try:
        # æµ‹è¯•1: ä½¿ç”¨å¯èƒ½ä¸å¯ç”¨çš„æ¨¡å‹
        workflow_with_fallback = EnhancedMultiAgentWorkflow(
            preferred_model_provider=ModelProvider.ANTHROPIC  # å¯èƒ½è¿æ¥å¤±è´¥
        )
        
        start_time = datetime.now()
        result = await workflow_with_fallback.execute_enhanced_workflow({
            "task": "å¤‡é€‰æœºåˆ¶æµ‹è¯•",
            "parameters": {"test_fallback": True}
        })
        execution_time = (datetime.now() - start_time).total_seconds()
        
        # å³ä½¿é¦–é€‰æ¨¡å‹å¤±è´¥ï¼Œå·¥ä½œæµåº”è¯¥ä»èƒ½å®Œæˆ
        logger.info(f"å¤‡é€‰æœºåˆ¶æµ‹è¯•ç»“æœ: {'æˆåŠŸ' if result.get('success') else 'å¤±è´¥'}")
        logger.info(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†å¤‡é€‰æ¨¡å‹
        llm_insights = result.get("llm_insights", {})
        if llm_insights:
            logger.info("âœ… å¤‡é€‰æœºåˆ¶å·¥ä½œæ­£å¸¸ï¼Œç”Ÿæˆäº†LLMæ´å¯Ÿ")
        else:
            logger.warning("âš ï¸ å¤‡é€‰æœºåˆ¶å¯èƒ½éœ€è¦ä¼˜åŒ–")
        
        return result.get("success", False)
        
    except Exception as e:
        logger.error(f"âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False


async def run_all_enhanced_multi_agent_tests():
    """è¿è¡Œæ‰€æœ‰AIå¢å¼ºç‰ˆMulti-Agentæµ‹è¯•"""
    
    logger.info("ğŸš€ å¼€å§‹è¿è¡ŒAIå¢å¼ºç‰ˆMulti-Agentç³»ç»Ÿå®Œæ•´æµ‹è¯•å¥—ä»¶")
    
    tests = [
        ("åŸºç¡€åŠŸèƒ½æµ‹è¯•", test_enhanced_workflow_basic),
        ("å®Œæ•´æ‰§è¡Œæµ‹è¯•", test_enhanced_workflow_execution),
        ("æ¨¡å‹æä¾›å•†å…¼å®¹æ€§æµ‹è¯•", test_different_model_providers),
        ("LLMå¢å¼ºåŠŸèƒ½è´¨é‡æµ‹è¯•", test_llm_enhancement_quality),
        ("é”™è¯¯å¤„ç†å’Œå¤‡é€‰æœºåˆ¶æµ‹è¯•", test_error_handling_and_fallback),
    ]
    
    success_count = 0
    total_tests = len(tests)
    
    for test_name, test_func in tests:
        try:
            logger.info(f"\n{'='*60}")
            logger.info(f"æ­£åœ¨è¿è¡Œ: {test_name}")
            logger.info(f"{'='*60}")
            
            result = await test_func()
            if result:
                logger.info(f"âœ… {test_name} - é€šè¿‡")
                success_count += 1
            else:
                logger.error(f"âŒ {test_name} - å¤±è´¥")
                
        except Exception as e:
            logger.error(f"âŒ {test_name} - å¼‚å¸¸: {e}")
    
    logger.info(f"\n{'='*60}")
    logger.info(f"AIå¢å¼ºç‰ˆMulti-Agentæµ‹è¯•å®Œæˆ: {success_count}/{total_tests} é€šè¿‡")
    logger.info(f"{'='*60}")
    
    if success_count == total_tests:
        logger.info("ğŸ‰ æ‰€æœ‰AIå¢å¼ºç‰ˆMulti-Agentæµ‹è¯•é€šè¿‡ï¼")
        return True
    else:
        logger.warning(f"âš ï¸  æœ‰ {total_tests - success_count} ä¸ªæµ‹è¯•å¤±è´¥")
        return False


if __name__ == "__main__":
    success = asyncio.run(run_all_enhanced_multi_agent_tests())
    sys.exit(0 if success else 1)