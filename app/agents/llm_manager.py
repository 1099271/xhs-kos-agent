"""
LLMæ¨¡å‹ç®¡ç†å™¨
ç»Ÿä¸€ç®¡ç†Anthropicã€OpenRouterç­‰å¤šç§LLMæ¨¡å‹çš„è®¿é—®
"""

import os
from typing import Optional, Dict, Any, List
from enum import Enum

from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage
from langchain_core.language_models import BaseChatModel

from app.config.settings import settings
from app.utils.logger import app_logger as logger


class ModelProvider(Enum):
    """æ¨¡å‹æä¾›å•†æšä¸¾"""

    ANTHROPIC = "anthropic"
    OPENROUTER = "openrouter"
    OPENAI = "openai"
    QWEN = "qwen"
    DEEPSEEK = "deepseek"


class LLMModelManager:
    """LLMæ¨¡å‹ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†å¤šç§æ¨¡å‹è®¿é—®"""

    def __init__(self):
        self.models: Dict[ModelProvider, Optional[BaseChatModel]] = {}
        self.default_provider = None
        self._initialize_models()

    def _initialize_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""

        # 1. å°è¯•åˆå§‹åŒ–Anthropic (é€šè¿‡anyrouter)
        if settings.ANTHROPIC_KEY and settings.ANTHROPIC_URL:
            try:
                self.models[ModelProvider.ANTHROPIC] = ChatAnthropic(
                    api_key=settings.ANTHROPIC_KEY,
                    base_url=settings.ANTHROPIC_URL,
                    model="claude-3-5-sonnet-20241022",
                    temperature=0.7,
                    max_tokens=4000,
                )
                if not self.default_provider:
                    self.default_provider = ModelProvider.ANTHROPIC
                logger.info("âœ… Anthropicæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ (é€šè¿‡anyrouter)")
            except Exception as e:
                logger.warning(f"âŒ Anthropicæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")

        # 2. å°è¯•åˆå§‹åŒ–OpenRouter
        if settings.OPENROUTER_KEY:
            try:
                self.models[ModelProvider.OPENROUTER] = ChatOpenAI(
                    api_key=settings.OPENROUTER_KEY,
                    base_url="https://openrouter.ai/api/v1",
                    model="anthropic/claude-3.5-sonnet",
                    temperature=0.7,
                    max_tokens=4000,
                )
                if not self.default_provider:
                    self.default_provider = ModelProvider.OPENROUTER
                logger.info("âœ… OpenRouteræ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âŒ OpenRouteræ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")

        # 3. å°è¯•åˆå§‹åŒ–OpenAI
        if settings.OPENAI_KEY:
            try:
                self.models[ModelProvider.OPENAI] = ChatOpenAI(
                    api_key=settings.OPENAI_KEY,
                    base_url="https://api.openai.com/v1",
                    model="gpt-4o-mini",
                    temperature=0.7,
                    max_tokens=4000,
                )
                if not self.default_provider:
                    self.default_provider = ModelProvider.OPENAI
                logger.info("âœ… OpenAIæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âŒ OpenAIæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")

        # 4. å°è¯•åˆå§‹åŒ–Qwen
        if settings.QWEN_MODEL_API_KEY:
            try:
                self.models[ModelProvider.QWEN] = ChatOpenAI(
                    api_key=settings.QWEN_MODEL_API_KEY,
                    base_url=settings.QWEN_MODEL_BASE_URL,
                    model=settings.QWEN_MODEL_NAME,
                    temperature=0.7,
                    max_tokens=4000,
                )
                if not self.default_provider:
                    self.default_provider = ModelProvider.QWEN
                logger.info("âœ… Qwenæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âŒ Qwenæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")

        # 5. å°è¯•åˆå§‹åŒ–DeepSeek
        if settings.DEEPSEEK_MODEL_API_KEY:
            try:
                self.models[ModelProvider.DEEPSEEK] = ChatOpenAI(
                    api_key=settings.DEEPSEEK_MODEL_API_KEY,
                    base_url=settings.DEEPSEEK_MODEL_BASE_URL,
                    model=settings.DEEPSEEK_MODEL_NAME,
                    temperature=0.7,
                    max_tokens=4000,
                )
                if not self.default_provider:
                    self.default_provider = ModelProvider.DEEPSEEK
                logger.info("âœ… DeepSeekæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âŒ DeepSeekæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")

        if self.default_provider:
            logger.info(f"ğŸ¯ é»˜è®¤æ¨¡å‹æä¾›å•†: {self.default_provider.value}")
        else:
            logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹æä¾›å•†ï¼è¯·æ£€æŸ¥é…ç½®")

    def get_model(
        self, provider: Optional[ModelProvider] = None
    ) -> Optional[BaseChatModel]:
        """è·å–æŒ‡å®šçš„æ¨¡å‹å®ä¾‹"""
        if provider is None:
            provider = self.default_provider

        if provider is None:
            logger.error("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹æä¾›å•†")
            return None

        return self.models.get(provider)

    def get_available_providers(self) -> List[ModelProvider]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹æä¾›å•†"""
        return [
            provider for provider, model in self.models.items() if model is not None
        ]

    async def invoke_with_fallback(
        self,
        messages: List[BaseMessage],
        preferred_provider: Optional[ModelProvider] = None,
    ) -> Optional[str]:
        """
        ä½¿ç”¨å¤‡é€‰æœºåˆ¶è°ƒç”¨æ¨¡å‹
        å¦‚æœé¦–é€‰æä¾›å•†å¤±è´¥ï¼Œä¼šè‡ªåŠ¨å°è¯•å…¶ä»–å¯ç”¨æä¾›å•†
        """

        providers_to_try = []

        # ç¡®å®šå°è¯•é¡ºåº
        if preferred_provider and preferred_provider in self.models:
            providers_to_try.append(preferred_provider)

        # æ·»åŠ å…¶ä»–å¯ç”¨æä¾›å•†ä½œä¸ºå¤‡é€‰
        for provider in [
            ModelProvider.ANTHROPIC,
            ModelProvider.OPENROUTER,
            ModelProvider.QWEN,
            ModelProvider.DEEPSEEK,
            ModelProvider.OPENAI,
        ]:
            if provider not in providers_to_try and provider in self.models:
                providers_to_try.append(provider)

        last_error = None

        for provider in providers_to_try:
            model = self.models.get(provider)
            if model is None:
                continue

            try:
                logger.info(f"ğŸ¤– å°è¯•ä½¿ç”¨ {provider.value} æ¨¡å‹")
                result = await model.ainvoke(messages)
                logger.info(f"âœ… {provider.value} æ¨¡å‹è°ƒç”¨æˆåŠŸ")
                return result.content if hasattr(result, "content") else str(result)

            except Exception as e:
                last_error = e
                logger.warning(f"âŒ {provider.value} æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
                continue

        logger.error(f"ğŸ’¥ æ‰€æœ‰æ¨¡å‹æä¾›å•†éƒ½å¤±è´¥äº†ï¼Œæœ€åä¸€ä¸ªé”™è¯¯: {last_error}")
        return None

    def create_prompt_messages(
        self, system_prompt: str, user_prompt: str, context: Optional[str] = None
    ) -> List[BaseMessage]:
        """åˆ›å»ºæ ‡å‡†çš„æç¤ºæ¶ˆæ¯æ ¼å¼"""

        messages = [SystemMessage(content=system_prompt)]

        if context:
            messages.append(HumanMessage(content=f"èƒŒæ™¯ä¿¡æ¯ï¼š\n{context}"))

        messages.append(HumanMessage(content=user_prompt))

        return messages


# å…¨å±€æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
llm_manager = LLMModelManager()


# ä¾¿æ·å‡½æ•°
async def call_llm(
    system_prompt: str,
    user_prompt: str,
    context: Optional[str] = None,
    preferred_provider: Optional[ModelProvider] = None,
) -> Optional[str]:
    """ä¾¿æ·çš„LLMè°ƒç”¨å‡½æ•°"""

    messages = llm_manager.create_prompt_messages(system_prompt, user_prompt, context)
    return await llm_manager.invoke_with_fallback(messages, preferred_provider)


async def call_llm_with_messages(
    messages: List[BaseMessage], preferred_provider: Optional[ModelProvider] = None
) -> Optional[str]:
    """ä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨è°ƒç”¨LLM"""

    return await llm_manager.invoke_with_fallback(messages, preferred_provider)


from app.prompts import prompt_manager
from app.utils.logger import app_logger as logger


class AgentLLMCaller:
    """ä¸ºAgentå®šåˆ¶çš„LLMè°ƒç”¨å™¨"""

    def __init__(
        self, agent_name: str, preferred_provider: Optional[ModelProvider] = None
    ):
        self.agent_name = agent_name
        self.preferred_provider = preferred_provider

    async def analyze_users(self, user_data: str, criteria: str) -> Optional[str]:
        """ç”¨æˆ·åˆ†æLLMè°ƒç”¨"""

        system_prompt = prompt_manager.format_prompt(
            "user_analyst_system", agent_name=self.agent_name
        )

        user_prompt = prompt_manager.format_prompt(
            "user_analyst_analysis", criteria=criteria, user_data=user_data
        )

        return await call_llm(
            system_prompt, user_prompt, preferred_provider=self.preferred_provider
        )

    async def create_content_strategy(
        self, user_profiles: str, business_goals: str
    ) -> Optional[str]:
        """å†…å®¹ç­–ç•¥åˆ¶å®šLLMè°ƒç”¨"""

        system_prompt = prompt_manager.format_prompt(
            "content_strategy_system", agent_name=self.agent_name
        )

        user_prompt = prompt_manager.format_prompt(
            "content_strategy_creation",
            user_profiles=user_profiles,
            business_goals=business_goals,
        )

        return await call_llm(
            system_prompt, user_prompt, preferred_provider=self.preferred_provider
        )

    async def generate_content(
        self, strategy: str, target_audience: str, themes: str
    ) -> Optional[str]:
        """å†…å®¹ç”ŸæˆLLMè°ƒç”¨"""

        system_prompt = prompt_manager.format_prompt(
            "content_generator_system", agent_name=self.agent_name
        )

        user_prompt = prompt_manager.format_prompt(
            "content_generator_creation",
            strategy=strategy,
            target_audience=target_audience,
            themes=themes,
        )

        return await call_llm(
            system_prompt, user_prompt, preferred_provider=self.preferred_provider
        )

    async def coordinate_strategy(
        self, all_agent_results: str, business_context: str
    ) -> Optional[str]:
        """ç­–ç•¥åè°ƒLLMè°ƒç”¨"""

        system_prompt = prompt_manager.format_prompt(
            "strategy_coordinator_system", agent_name=self.agent_name
        )

        user_prompt = prompt_manager.format_prompt(
            "strategy_coordinator_coordination",
            all_agent_results=all_agent_results,
            business_context=business_context,
        )

        return await call_llm(
            system_prompt, user_prompt, preferred_provider=self.preferred_provider
        )
