"""
LlamaIndexé›†æˆæ¨¡å—
æä¾›æ™ºèƒ½æ–‡æ¡£ç´¢å¼•ã€è¯­ä¹‰æœç´¢å’ŒçŸ¥è¯†æ£€ç´¢åŠŸèƒ½
"""

import os
import asyncio
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
from pathlib import Path

from llama_index.core import VectorStoreIndex, Document, Settings, StorageContext
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.indices.postprocessor import SimilarityPostprocessor

try:
    from llama_index.embeddings.openai import OpenAIEmbedding
    from llama_index.llms.openai import OpenAI
except ImportError:
    # Fallback for different llama-index versions
    try:
        from llama_index.llms.openai import OpenAI
        from llama_index.embeddings.openai import OpenAIEmbedding
    except ImportError:
        OpenAI = None
        OpenAIEmbedding = None

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, text
from app.infra.db.async_database import get_session_context
from app.infra.models.comment_models import XhsComment
from app.infra.models.note_models import XhsNote
from app.infra.models.llm_models import LlmCommentAnalysis
from app.config.settings import settings
from app.utils.logger import app_logger as logger


class LlamaIndexManager:
    """LlamaIndexç®¡ç†å™¨ - æä¾›æ™ºèƒ½æ–‡æ¡£ç´¢å¼•å’Œæ£€ç´¢åŠŸèƒ½"""

    def __init__(self, persist_dir: str = "./storage"):
        self.persist_dir = Path(persist_dir)
        self.persist_dir.mkdir(exist_ok=True)

        # ç´¢å¼•å­˜å‚¨
        self.comment_index: Optional[VectorStoreIndex] = None
        self.note_index: Optional[VectorStoreIndex] = None
        self.analysis_index: Optional[VectorStoreIndex] = None

        # é…ç½®LlamaIndexè®¾ç½®
        self._setup_llamaindex_settings()

        # åˆå§‹åŒ–å­˜å‚¨ä¸Šä¸‹æ–‡
        self._init_storage_context()

    def _setup_llamaindex_settings(self):
        """é…ç½®LlamaIndexå…¨å±€è®¾ç½®"""

        # è®¾ç½®åµŒå…¥æ¨¡å‹
        try:
            # ä¼˜å…ˆä½¿ç”¨OpenAIåµŒå…¥æ¨¡å‹
            if settings.OPENAI_KEY and OpenAIEmbedding:
                Settings.embed_model = OpenAIEmbedding(
                    api_key=settings.OPENAI_KEY, model="text-embedding-3-small"
                )
                logger.info("âœ… ä½¿ç”¨OpenAIåµŒå…¥æ¨¡å‹")
            else:
                logger.warning("âš ï¸ æœªé…ç½®åµŒå…¥æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")

        except Exception as e:
            logger.warning(f"âš ï¸ åµŒå…¥æ¨¡å‹è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹: {e}")

        # è®¾ç½®LLM
        try:
            if settings.OPENAI_KEY and OpenAI:
                Settings.llm = OpenAI(
                    api_key=settings.OPENAI_KEY, model="gpt-3.5-turbo"
                )
                logger.info("âœ… ä½¿ç”¨OpenAI LLM")
        except Exception as e:
            logger.warning(f"âš ï¸ LLMè®¾ç½®å¤±è´¥: {e}")

        # è®¾ç½®æ–‡æœ¬åˆ†å‰²å™¨
        Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)

    def _init_storage_context(self):
        """åˆå§‹åŒ–å­˜å‚¨ä¸Šä¸‹æ–‡"""

        # ä¸ºä¸åŒç±»å‹çš„æ•°æ®åˆ›å»ºç‹¬ç«‹çš„å­˜å‚¨
        self.comment_storage_dir = self.persist_dir / "comments"
        self.note_storage_dir = self.persist_dir / "notes"
        self.analysis_storage_dir = self.persist_dir / "analysis"

        for storage_dir in [
            self.comment_storage_dir,
            self.note_storage_dir,
            self.analysis_storage_dir,
        ]:
            storage_dir.mkdir(exist_ok=True)

    async def build_comment_index(self, limit: int = 1000) -> bool:
        """æ„å»ºè¯„è®ºæ•°æ®ç´¢å¼•"""

        logger.info(f"ğŸ” å¼€å§‹æ„å»ºè¯„è®ºæ•°æ®ç´¢å¼• (é™åˆ¶: {limit}æ¡)")

        try:
            # ä»æ•°æ®åº“è·å–è¯„è®ºæ•°æ®
            async with get_session_context() as session:
                query = select(XhsComment).limit(limit)
                result = await session.execute(query)
                comments = result.scalars().all()

            if not comments:
                logger.warning("âŒ æ²¡æœ‰æ‰¾åˆ°è¯„è®ºæ•°æ®")
                return False

            # è½¬æ¢ä¸ºLlamaIndexæ–‡æ¡£
            documents = []
            for comment in comments:
                content = f"""
è¯„è®ºID: {comment.comment_id}
ç¬”è®°ID: {comment.note_id}
ç”¨æˆ·æ˜µç§°: {comment.comment_user_nickname}
è¯„è®ºå†…å®¹: {comment.comment_content}
ç‚¹èµæ•°: {comment.comment_like_count}
åˆ›å»ºæ—¶é—´: {comment.comment_create_time}
"""

                doc = Document(
                    text=content,
                    metadata={
                        "comment_id": comment.comment_id,
                        "note_id": comment.note_id,
                        "user_id": comment.comment_user_id,
                        "user_nickname": comment.comment_user_nickname,
                        "like_count": comment.comment_like_count,
                        "create_time": str(comment.comment_create_time),
                        "type": "comment",
                    },
                )
                documents.append(doc)

            # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
            storage_context = StorageContext.from_defaults(
                persist_dir=str(self.comment_storage_dir)
            )

            # æ„å»ºç´¢å¼•
            self.comment_index = VectorStoreIndex.from_documents(
                documents, storage_context=storage_context, show_progress=True
            )

            # æŒä¹…åŒ–ç´¢å¼•
            self.comment_index.storage_context.persist(
                persist_dir=str(self.comment_storage_dir)
            )

            logger.info(f"âœ… è¯„è®ºç´¢å¼•æ„å»ºå®Œæˆï¼Œå…±å¤„ç† {len(documents)} æ¡è¯„è®º")
            return True

        except Exception as e:
            logger.error(f"âŒ æ„å»ºè¯„è®ºç´¢å¼•å¤±è´¥: {e}")
            return False

    async def build_note_index(self, limit: int = 500) -> bool:
        """æ„å»ºç¬”è®°æ•°æ®ç´¢å¼•"""

        logger.info(f"ğŸ“ å¼€å§‹æ„å»ºç¬”è®°æ•°æ®ç´¢å¼• (é™åˆ¶: {limit}æ¡)")

        try:
            # ä»æ•°æ®åº“è·å–ç¬”è®°æ•°æ®
            async with get_session_context() as session:
                query = select(XhsNote).limit(limit)
                result = await session.execute(query)
                notes = result.scalars().all()

            if not notes:
                logger.warning("âŒ æ²¡æœ‰æ‰¾åˆ°ç¬”è®°æ•°æ®")
                return False

            # è½¬æ¢ä¸ºLlamaIndexæ–‡æ¡£
            documents = []
            for note in notes:
                content = f"""
ç¬”è®°ID: {note.note_id}
æ ‡é¢˜: {note.note_display_title}
ä½œè€…ID: {note.author_user_id}
ç‚¹èµæ•°: {note.note_liked_count}
å¡ç‰‡ç±»å‹: {note.note_card_type}
æ¨¡å‹ç±»å‹: {note.note_model_type}
åˆ›å»ºæ—¶é—´: {note.created_at if hasattr(note, 'created_at') else 'N/A'}
"""

                doc = Document(
                    text=content,
                    metadata={
                        "note_id": note.note_id,
                        "title": note.note_display_title,
                        "author_user_id": note.author_user_id,
                        "liked_count": note.note_liked_count,
                        "card_type": note.note_card_type,
                        "model_type": note.note_model_type,
                        "created_at": (
                            str(note.created_at)
                            if hasattr(note, "created_at")
                            else "N/A"
                        ),
                        "type": "note",
                    },
                )
                documents.append(doc)

            # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
            storage_context = StorageContext.from_defaults(
                persist_dir=str(self.note_storage_dir)
            )

            # æ„å»ºç´¢å¼•
            self.note_index = VectorStoreIndex.from_documents(
                documents, storage_context=storage_context, show_progress=True
            )

            # æŒä¹…åŒ–ç´¢å¼•
            self.note_index.storage_context.persist(
                persist_dir=str(self.note_storage_dir)
            )

            logger.info(f"âœ… ç¬”è®°ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…±å¤„ç† {len(documents)} æ¡ç¬”è®°")
            return True

        except Exception as e:
            logger.error(f"âŒ æ„å»ºç¬”è®°ç´¢å¼•å¤±è´¥: {e}")
            return False

    async def build_analysis_index(self, limit: int = 1000) -> bool:
        """æ„å»ºLLMåˆ†ææ•°æ®ç´¢å¼•"""

        logger.info(f"ğŸ§  å¼€å§‹æ„å»ºLLMåˆ†ææ•°æ®ç´¢å¼• (é™åˆ¶: {limit}æ¡)")

        try:
            # ä»æ•°æ®åº“è·å–LLMåˆ†ææ•°æ®
            async with get_session_context() as session:
                query = select(LlmCommentAnalysis).limit(limit)
                result = await session.execute(query)
                analyses = result.scalars().all()

            if not analyses:
                logger.warning("âŒ æ²¡æœ‰æ‰¾åˆ°LLMåˆ†ææ•°æ®")
                return False

            # è½¬æ¢ä¸ºLlamaIndexæ–‡æ¡£
            documents = []
            for analysis in analyses:
                content = f"""
åˆ†æID: {analysis.id}
è¯„è®ºID: {analysis.comment_id}
ç¬”è®°ID: {analysis.note_id}
ç”¨æˆ·æ˜µç§°: {analysis.comment_user_nickname}
æƒ…æ„Ÿå€¾å‘: {analysis.emotional_preference}
AIPSåå¥½: {analysis.aips_preference}
æ˜¯å¦å»è¿‡: {analysis.has_visited}
æœªæ»¡è¶³éœ€æ±‚: {analysis.unmet_preference}
æœªæ»¡è¶³éœ€æ±‚æè¿°: {analysis.unmet_desc}
æ€§åˆ«: {analysis.gender}
å¹´é¾„: {analysis.age}
åˆ†ææ—¶é—´: {analysis.created_at}
"""

                doc = Document(
                    text=content,
                    metadata={
                        "analysis_id": analysis.id,
                        "comment_id": analysis.comment_id,
                        "note_id": analysis.note_id,
                        "user_id": analysis.comment_user_id,
                        "user_nickname": analysis.comment_user_nickname,
                        "emotional_preference": analysis.emotional_preference,
                        "aips_preference": analysis.aips_preference,
                        "has_visited": analysis.has_visited,
                        "unmet_preference": analysis.unmet_preference,
                        "gender": analysis.gender,
                        "age": analysis.age,
                        "created_at": str(analysis.created_at),
                        "type": "analysis",
                    },
                )
                documents.append(doc)

            # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
            storage_context = StorageContext.from_defaults(
                persist_dir=str(self.analysis_storage_dir)
            )

            # æ„å»ºç´¢å¼•
            self.analysis_index = VectorStoreIndex.from_documents(
                documents, storage_context=storage_context, show_progress=True
            )

            # æŒä¹…åŒ–ç´¢å¼•
            self.analysis_index.storage_context.persist(
                persist_dir=str(self.analysis_storage_dir)
            )

            logger.info(f"âœ… LLMåˆ†æç´¢å¼•æ„å»ºå®Œæˆï¼Œå…±å¤„ç† {len(documents)} æ¡åˆ†æ")
            return True

        except Exception as e:
            logger.error(f"âŒ æ„å»ºLLMåˆ†æç´¢å¼•å¤±è´¥: {e}")
            return False

    def load_existing_indexes(self) -> bool:
        """åŠ è½½å·²å­˜åœ¨çš„ç´¢å¼•"""

        logger.info("ğŸ“‚ åŠ è½½å·²å­˜åœ¨çš„ç´¢å¼•")

        try:
            # åŠ è½½è¯„è®ºç´¢å¼•
            if (self.comment_storage_dir / "docstore.json").exists():
                try:
                    storage_context = StorageContext.from_defaults(
                        persist_dir=str(self.comment_storage_dir)
                    )
                    self.comment_index = VectorStoreIndex.from_documents(
                        [], storage_context=storage_context
                    )
                    logger.info("âœ… è¯„è®ºç´¢å¼•åŠ è½½æˆåŠŸ")
                except Exception as e:
                    logger.warning(f"âš ï¸ è¯„è®ºç´¢å¼•åŠ è½½å¤±è´¥: {e}")

            # åŠ è½½ç¬”è®°ç´¢å¼•
            if (self.note_storage_dir / "docstore.json").exists():
                try:
                    storage_context = StorageContext.from_defaults(
                        persist_dir=str(self.note_storage_dir)
                    )
                    self.note_index = VectorStoreIndex.from_documents(
                        [], storage_context=storage_context
                    )
                    logger.info("âœ… ç¬”è®°ç´¢å¼•åŠ è½½æˆåŠŸ")
                except Exception as e:
                    logger.warning(f"âš ï¸ ç¬”è®°ç´¢å¼•åŠ è½½å¤±è´¥: {e}")

            # åŠ è½½åˆ†æç´¢å¼•
            if (self.analysis_storage_dir / "docstore.json").exists():
                try:
                    storage_context = StorageContext.from_defaults(
                        persist_dir=str(self.analysis_storage_dir)
                    )
                    self.analysis_index = VectorStoreIndex.from_documents(
                        [], storage_context=storage_context
                    )
                    logger.info("âœ… LLMåˆ†æç´¢å¼•åŠ è½½æˆåŠŸ")
                except Exception as e:
                    logger.warning(f"âš ï¸ LLMåˆ†æç´¢å¼•åŠ è½½å¤±è´¥: {e}")

            return True

        except Exception as e:
            logger.error(f"âŒ åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
            return False

    async def semantic_search(
        self,
        query: str,
        index_type: str = "all",
        top_k: int = 5,
        similarity_threshold: float = 0.7,
    ) -> List[Dict[str, Any]]:
        """è¯­ä¹‰æœç´¢"""

        logger.info(f"ğŸ” æ‰§è¡Œè¯­ä¹‰æœç´¢: '{query}' (ç±»å‹: {index_type}, Top-K: {top_k})")

        results = []

        try:
            # é€‰æ‹©è¦æœç´¢çš„ç´¢å¼•
            indexes_to_search = []

            if index_type == "all" or index_type == "comment":
                if self.comment_index:
                    indexes_to_search.append(("comment", self.comment_index))

            if index_type == "all" or index_type == "note":
                if self.note_index:
                    indexes_to_search.append(("note", self.note_index))

            if index_type == "all" or index_type == "analysis":
                if self.analysis_index:
                    indexes_to_search.append(("analysis", self.analysis_index))

            # å¯¹æ¯ä¸ªç´¢å¼•æ‰§è¡Œæœç´¢
            for index_name, index in indexes_to_search:
                try:
                    # åˆ›å»ºæ£€ç´¢å™¨
                    retriever = VectorIndexRetriever(
                        index=index, similarity_top_k=top_k
                    )

                    # æ‰§è¡Œæ£€ç´¢
                    nodes = retriever.retrieve(query)

                    # å¤„ç†ç»“æœ
                    for node in nodes:
                        # ä¿®å¤: å¤„ç† node.score å¯èƒ½ä¸º None çš„æƒ…å†µ
                        if (
                            node.score is not None
                            and node.score >= similarity_threshold
                        ):
                            result = {
                                "index_type": index_name,
                                "content": node.text,
                                "metadata": node.metadata,
                                "score": node.score,
                                "node_id": node.node_id,
                            }
                            results.append(result)

                except Exception as e:
                    logger.warning(f"âš ï¸ æœç´¢ç´¢å¼• {index_name} å¤±è´¥: {e}")

            # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•°æ’åº
            results.sort(key=lambda x: x["score"], reverse=True)

            logger.info(f"âœ… è¯­ä¹‰æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")
            return results[:top_k]

        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return []

    async def intelligent_query(
        self, question: str, context_type: str = "all", max_context_length: int = 2000
    ) -> Optional[str]:
        """æ™ºèƒ½é—®ç­” - åŸºäºç´¢å¼•å†…å®¹å›ç­”é—®é¢˜"""

        logger.info(f"â“ æ™ºèƒ½é—®ç­”: '{question}' (ä¸Šä¸‹æ–‡ç±»å‹: {context_type})")

        try:
            # é¦–å…ˆè¿›è¡Œè¯­ä¹‰æœç´¢è·å–ç›¸å…³å†…å®¹
            search_results = await self.semantic_search(
                query=question,
                index_type=context_type,
                top_k=3,
                similarity_threshold=0.6,
            )

            if not search_results:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹")
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚"

            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            current_length = 0

            for result in search_results:
                content = result["content"]
                if current_length + len(content) <= max_context_length:
                    context_parts.append(f"[{result['index_type']}] {content}")
                    current_length += len(content)
                else:
                    break

            context = "\n\n".join(context_parts)

            # ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
            from app.agents.llm_manager import call_llm

            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æåŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¯·æ³¨æ„ï¼š
1. åªåŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å›ç­”è¦å‡†ç¡®ã€ç®€æ´ã€æœ‰ç”¨
4. å¯ä»¥è¿›è¡Œåˆç†çš„åˆ†æå’Œæ¨ç†"""

            user_prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ã€‚"""

            answer = await call_llm(system_prompt, user_prompt)

            if answer:
                logger.info("âœ… æ™ºèƒ½é—®ç­”å®Œæˆ")
                return answer
            else:
                logger.warning("âš ï¸ LLMå›ç­”ç”Ÿæˆå¤±è´¥")
                return "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›ç­”ï¼Œè¯·ç¨åé‡è¯•ã€‚"

        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½é—®ç­”å¤±è´¥: {e}")
            return f"æŸ¥è¯¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"

    async def get_user_insights(self, user_id: str) -> Dict[str, Any]:
        """è·å–ç‰¹å®šç”¨æˆ·çš„æ·±åº¦æ´å¯Ÿ"""

        logger.info(f"ğŸ‘¤ è·å–ç”¨æˆ·æ´å¯Ÿ: {user_id}")

        try:
            # æœç´¢è¯¥ç”¨æˆ·çš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯
            user_query = f"ç”¨æˆ·ID:{user_id} OR comment_user_id:{user_id}"

            search_results = await self.semantic_search(
                query=user_query, index_type="all", top_k=10, similarity_threshold=0.5
            )

            # åˆ†ç±»æ•´ç†ç»“æœ
            comments = []
            notes = []
            analyses = []

            for result in search_results:
                if result["index_type"] == "comment":
                    comments.append(result)
                elif result["index_type"] == "note":
                    notes.append(result)
                elif result["index_type"] == "analysis":
                    analyses.append(result)

            # ç”Ÿæˆç”¨æˆ·æ´å¯ŸæŠ¥å‘Š
            insights = {
                "user_id": user_id,
                "total_records": len(search_results),
                "comments_count": len(comments),
                "notes_count": len(notes),
                "analyses_count": len(analyses),
                "comments": comments,
                "notes": notes,
                "analyses": analyses,
                "summary": f"ç”¨æˆ· {user_id} å…±æœ‰ {len(search_results)} æ¡ç›¸å…³è®°å½•",
                "generated_at": datetime.now().isoformat(),
            }

            logger.info(f"âœ… ç”¨æˆ·æ´å¯Ÿè·å–å®Œæˆ: {len(search_results)} æ¡è®°å½•")
            return insights

        except Exception as e:
            logger.error(f"âŒ è·å–ç”¨æˆ·æ´å¯Ÿå¤±è´¥: {e}")
            return {"error": str(e)}

    async def build_all_indexes(self) -> Dict[str, bool]:
        """æ„å»ºæ‰€æœ‰ç´¢å¼•"""

        logger.info("ğŸ—ï¸ å¼€å§‹æ„å»ºæ‰€æœ‰ç´¢å¼•")

        results = {}

        # æ„å»ºè¯„è®ºç´¢å¼•
        results["comment_index"] = await self.build_comment_index()

        # æ„å»ºç¬”è®°ç´¢å¼•
        results["note_index"] = await self.build_note_index()

        # æ„å»ºåˆ†æç´¢å¼•
        results["analysis_index"] = await self.build_analysis_index()

        success_count = sum(results.values())
        total_count = len(results)

        logger.info(f"ğŸ‰ ç´¢å¼•æ„å»ºå®Œæˆ: {success_count}/{total_count} ä¸ªç´¢å¼•æˆåŠŸ")

        return results


# å…¨å±€LlamaIndexç®¡ç†å™¨å®ä¾‹
llamaindex_manager = LlamaIndexManager()
