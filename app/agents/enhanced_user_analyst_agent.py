"""
å¢å¼ºç‰ˆç”¨æˆ·åˆ†æAgent
é›†æˆLlamaIndexæ™ºèƒ½æ£€ç´¢åŠŸèƒ½ï¼Œæä¾›æ›´æ·±åº¦çš„ç”¨æˆ·æ´å¯Ÿåˆ†æ
"""

from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import asyncio

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, and_, or_, desc
from sqlalchemy.orm import selectinload

from app.agents.user_analyst_agent import UserAnalystAgent, UserProfile, AnalysisResult
from app.agents.llamaindex_manager import LlamaIndexManager
from app.agents.llm_manager import AgentLLMCaller, ModelProvider
from app.prompts.user_analyst_prompts import get_user_analyst_prompt
from app.infra.models.llm_models import LlmCommentAnalysis
from app.infra.models.comment_models import XhsComment
from app.utils.logger import app_logger as logger


@dataclass
class EnhancedUserProfile(UserProfile):
    """å¢å¼ºç‰ˆç”¨æˆ·ç”»åƒ - åŒ…å«LlamaIndexæ£€ç´¢ç»“æœ"""
    semantic_search_results: List[Dict[str, Any]]
    related_content_summary: str
    ai_insights: str
    retrieval_score: float


@dataclass
class EnhancedAnalysisResult(AnalysisResult):
    """å¢å¼ºç‰ˆåˆ†æç»“æœ - åŒ…å«æ™ºèƒ½æ£€ç´¢æ´å¯Ÿ"""
    semantic_insights: Dict[str, Any]
    content_analysis: Dict[str, Any]
    retrieval_summary: str


class EnhancedUserAnalystAgent(UserAnalystAgent):
    """å¢å¼ºç‰ˆç”¨æˆ·åˆ†æAgent - é›†æˆLlamaIndexæ™ºèƒ½æ£€ç´¢"""
    
    def __init__(self, preferred_model_provider: Optional[ModelProvider] = None):
        super().__init__()
        self.name = "EnhancedUserAnalystAgent"
        self.llamaindex_manager = LlamaIndexManager()
        self.llm_caller = AgentLLMCaller(self.name, preferred_model_provider)
        
        # å°è¯•åŠ è½½å·²æœ‰ç´¢å¼•
        self.llamaindex_manager.load_existing_indexes()
    
    async def execute_enhanced_analysis(
        self, 
        db_session: AsyncSession, 
        criteria: Optional[Dict[str, Any]] = None
    ) -> EnhancedAnalysisResult:
        """
        æ‰§è¡Œå¢å¼ºç‰ˆç”¨æˆ·åˆ†æï¼Œç»“åˆä¼ ç»Ÿæ•°æ®åˆ†æå’Œæ™ºèƒ½æ£€ç´¢
        
        Args:
            db_session: æ•°æ®åº“ä¼šè¯
            criteria: ç­›é€‰æ¡ä»¶
            
        Returns:
            EnhancedAnalysisResult: å¢å¼ºç‰ˆåˆ†æç»“æœ
        """
        start_time = datetime.now()
        
        # æ‰§è¡ŒåŸºç¡€ç”¨æˆ·åˆ†æ
        logger.info("ğŸ” æ‰§è¡ŒåŸºç¡€ç”¨æˆ·åˆ†æ...")
        basic_result = await self.execute(db_session, criteria)
        
        # æ‰§è¡Œè¯­ä¹‰æ£€ç´¢å¢å¼º
        logger.info("ğŸ§  æ‰§è¡Œè¯­ä¹‰æ£€ç´¢å¢å¼º...")
        semantic_insights = await self._perform_semantic_analysis(basic_result, criteria)
        
        # æ‰§è¡Œå†…å®¹åˆ†æ
        logger.info("ğŸ“ æ‰§è¡Œç”¨æˆ·å†…å®¹åˆ†æ...")
        content_analysis = await self._perform_content_analysis(basic_result.high_value_users)
        
        # ç”Ÿæˆæ£€ç´¢æ‘˜è¦
        logger.info("ğŸ“Š ç”Ÿæˆæ™ºèƒ½åˆ†ææ‘˜è¦...")
        retrieval_summary = await self._generate_retrieval_summary(
            semantic_insights, content_analysis, basic_result
        )
        
        # å¢å¼ºç”¨æˆ·ç”»åƒ
        enhanced_users = await self._enhance_user_profiles(
            db_session, basic_result.high_value_users
        )
        
        # æ„å»ºå¢å¼ºç‰ˆç»“æœ
        enhanced_result = EnhancedAnalysisResult(
            high_value_users=enhanced_users,
            total_analyzed=basic_result.total_analyzed,
            analysis_time=start_time,
            criteria_used=basic_result.criteria_used,
            semantic_insights=semantic_insights,
            content_analysis=content_analysis,
            retrieval_summary=retrieval_summary
        )
        
        logger.info(f"âœ… å¢å¼ºç‰ˆç”¨æˆ·åˆ†æå®Œæˆï¼Œå¤„ç†äº† {len(enhanced_users)} ä¸ªç”¨æˆ·")
        return enhanced_result
    
    async def _perform_semantic_analysis(
        self, 
        basic_result: AnalysisResult, 
        criteria: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """æ‰§è¡Œè¯­ä¹‰åˆ†æ"""
        
        try:
            # æ„å»ºè¯­ä¹‰æœç´¢æŸ¥è¯¢
            search_queries = self._build_semantic_queries(criteria)
            
            semantic_results = {}
            
            for query_name, query_text in search_queries.items():
                logger.info(f"æ‰§è¡Œè¯­ä¹‰æœç´¢: {query_name}")
                
                search_results = await self.llamaindex_manager.semantic_search(
                    query=query_text,
                    index_type="all",
                    top_k=5,
                    similarity_threshold=0.6
                )
                
                semantic_results[query_name] = {
                    "query": query_text,
                    "results": search_results,
                    "result_count": len(search_results)
                }
            
            # ç”Ÿæˆè¯­ä¹‰æ´å¯Ÿæ‘˜è¦
            insights_summary = await self._generate_semantic_insights_summary(semantic_results)
            
            return {
                "search_results": semantic_results,
                "insights_summary": insights_summary,
                "total_queries": len(search_queries),
                "execution_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰åˆ†æå¤±è´¥: {e}")
            return {"error": str(e), "search_results": {}}
    
    def _build_semantic_queries(self, criteria: Optional[Dict[str, Any]]) -> Dict[str, str]:
        """æ„å»ºè¯­ä¹‰æœç´¢æŸ¥è¯¢"""
        
        queries = {
            "é«˜ä»·å€¼ç”¨æˆ·ç‰¹å¾": "é«˜ä»·å€¼ç”¨æˆ· é«˜äº’åŠ¨ æ­£å‘æƒ…æ„Ÿ æ¶ˆè´¹èƒ½åŠ›å¼º",
            "ç”¨æˆ·è¡Œä¸ºæ¨¡å¼": "ç”¨æˆ·è¡Œä¸º è¯„è®ºä¹ æƒ¯ äº’åŠ¨æ–¹å¼ æ´»è·ƒæ—¶é—´",
            "å†…å®¹åå¥½åˆ†æ": "å†…å®¹åå¥½ å…´è¶£è¯é¢˜ å…³æ³¨é¢†åŸŸ å‚ä¸åº¦",
            "æƒ…æ„Ÿå€¾å‘åˆ†å¸ƒ": "æƒ…æ„Ÿå€¾å‘ æ­£å‘ è´Ÿå‘ ä¸­æ€§ ç”¨æˆ·æƒ…ç»ª",
            "æœªæ»¡è¶³éœ€æ±‚": "æœªæ»¡è¶³éœ€æ±‚ ç”¨æˆ·ç—›ç‚¹ éœ€æ±‚ç¼ºå£ æ”¹è¿›å»ºè®®"
        }
        
        # æ ¹æ®ç­›é€‰æ¡ä»¶è°ƒæ•´æŸ¥è¯¢
        if criteria:
            if criteria.get("emotional_preference"):
                emotions = " ".join(criteria["emotional_preference"])
                queries["ç›®æ ‡æƒ…æ„Ÿç¾¤ä½“"] = f"æƒ…æ„Ÿå€¾å‘ {emotions} ç”¨æˆ·ç‰¹å¾"
            
            if criteria.get("unmet_preference"):
                queries["éœ€æ±‚åˆ†æ"] = "æœªæ»¡è¶³éœ€æ±‚ ç”¨æˆ·æœŸæœ› æ”¹è¿›ç©ºé—´"
            
            if criteria.get("exclude_visited"):
                queries["æ½œåœ¨å®¢æˆ·"] = "æœªå»è¿‡ æ½œåœ¨å®¢æˆ· æ–°ç”¨æˆ· è·å®¢æœºä¼š"
        
        return queries
    
    async def _generate_semantic_insights_summary(
        self, 
        semantic_results: Dict[str, Any]
    ) -> str:
        """ç”Ÿæˆè¯­ä¹‰æ´å¯Ÿæ‘˜è¦"""
        
        try:
            # æ•´ç†æœç´¢ç»“æœ
            all_results = []
            for query_name, query_data in semantic_results.items():
                results = query_data.get("results", [])
                all_results.extend([
                    f"æŸ¥è¯¢: {query_name}\nç»“æœ: {r['content'][:200]}...\nåˆ†æ•°: {r['score']:.3f}"
                    for r in results[:2]  # æ¯ä¸ªæŸ¥è¯¢å–å‰2ä¸ªç»“æœ
                ])
            
            if not all_results:
                return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„è¯­ä¹‰æœç´¢ç»“æœ"
            
            # ä½¿ç”¨LLMç”Ÿæˆæ´å¯Ÿæ‘˜è¦
            prompt_template = get_user_analyst_prompt("deep_user_analysis")
            
            analysis_prompt = prompt_template.format(
                user_basic_info="åŸºäºè¯­ä¹‰æœç´¢çš„ç”¨æˆ·åˆ†æ",
                user_behavior_data="\n\n".join(all_results[:5]),  # é™åˆ¶é•¿åº¦
                comment_sentiment_data="å¤šç»´åº¦è¯­ä¹‰æœç´¢ç»“æœ"
            )
            
            insights = await self.llm_caller.analyze_users(
                analysis_prompt, 
                "ç”ŸæˆåŸºäºè¯­ä¹‰æœç´¢ç»“æœçš„ç”¨æˆ·æ´å¯Ÿæ‘˜è¦"
            )
            
            return insights if insights else "æ— æ³•ç”Ÿæˆè¯­ä¹‰æ´å¯Ÿæ‘˜è¦"
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆè¯­ä¹‰æ´å¯Ÿæ‘˜è¦å¤±è´¥: {e}")
            return f"è¯­ä¹‰æ´å¯Ÿæ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    async def _perform_content_analysis(
        self, 
        high_value_users: List[UserProfile]
    ) -> Dict[str, Any]:
        """æ‰§è¡Œç”¨æˆ·å†…å®¹åˆ†æ"""
        
        try:
            if not high_value_users:
                return {"message": "æ²¡æœ‰é«˜ä»·å€¼ç”¨æˆ·æ•°æ®è¿›è¡Œå†…å®¹åˆ†æ"}
            
            content_insights = {}
            
            # åˆ†æå‰å‡ ä¸ªé«˜ä»·å€¼ç”¨æˆ·çš„å†…å®¹
            for i, user in enumerate(high_value_users[:5], 1):
                logger.info(f"åˆ†æç”¨æˆ· {user.user_id} çš„å†…å®¹...")
                
                # ä½¿ç”¨LlamaIndexè·å–ç”¨æˆ·è¯¦ç»†æ´å¯Ÿ
                user_insights = await self.llamaindex_manager.get_user_insights(user.user_id)
                
                if "error" not in user_insights:
                    content_insights[f"user_{i}"] = {
                        "user_id": user.user_id,
                        "nickname": user.nickname,
                        "insights": user_insights,
                        "content_summary": self._summarize_user_content(user_insights)
                    }
            
            # ç”Ÿæˆæ•´ä½“å†…å®¹åˆ†æ
            overall_analysis = await self._generate_content_analysis_summary(content_insights)
            
            return {
                "individual_insights": content_insights,
                "overall_analysis": overall_analysis,
                "analyzed_users_count": len(content_insights),
                "analysis_timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ å†…å®¹åˆ†æå¤±è´¥: {e}")
            return {"error": str(e)}
    
    def _summarize_user_content(self, user_insights: Dict[str, Any]) -> str:
        """æ€»ç»“ç”¨æˆ·å†…å®¹"""
        
        try:
            summary_parts = []
            
            # æ€»ç»“æ•°æ®é‡
            summary_parts.append(f"æ€»è®°å½•: {user_insights.get('total_records', 0)}")
            summary_parts.append(f"è¯„è®º: {user_insights.get('comments_count', 0)}")
            summary_parts.append(f"ç¬”è®°: {user_insights.get('notes_count', 0)}")
            summary_parts.append(f"åˆ†æ: {user_insights.get('analyses_count', 0)}")
            
            # æ·»åŠ å…³é”®æ´å¯Ÿ
            if user_insights.get('analyses'):
                analyses = user_insights['analyses']
                if analyses:
                    first_analysis = analyses[0]
                    metadata = first_analysis.get('metadata', {})
                    if metadata.get('emotional_preference'):
                        summary_parts.append(f"æƒ…æ„Ÿå€¾å‘: {metadata['emotional_preference']}")
                    if metadata.get('unmet_preference'):
                        summary_parts.append(f"æœªæ»¡è¶³éœ€æ±‚: {metadata['unmet_preference']}")
            
            return " | ".join(summary_parts)
            
        except Exception as e:
            logger.error(f"ç”¨æˆ·å†…å®¹æ€»ç»“å¤±è´¥: {e}")
            return "å†…å®¹æ€»ç»“ç”Ÿæˆå¤±è´¥"
    
    async def _generate_content_analysis_summary(
        self, 
        content_insights: Dict[str, Any]
    ) -> str:
        """ç”Ÿæˆå†…å®¹åˆ†ææ€»ç»“"""
        
        try:
            if not content_insights:
                return "æ²¡æœ‰å†…å®¹æ´å¯Ÿæ•°æ®"
            
            # æ•´ç†ç”¨æˆ·å†…å®¹æ•°æ®
            summary_data = []
            for user_key, user_data in content_insights.items():
                summary_data.append(
                    f"ç”¨æˆ· {user_data['user_id']} ({user_data['nickname']}): "
                    f"{user_data['content_summary']}"
                )
            
            content_summary = "\n".join(summary_data)
            
            # ä½¿ç”¨LLMç”Ÿæˆæ·±åº¦åˆ†æ
            prompt_template = get_user_analyst_prompt("user_journey_mapping")
            
            analysis_prompt = prompt_template.format(
                touchpoint_data=content_summary,
                conversion_funnel_data="ç”¨æˆ·å†…å®¹å‚ä¸å’Œäº’åŠ¨æ•°æ®",
                user_feedback_data="ç”¨æˆ·è¯„è®ºå’Œåˆ†ææ•°æ®"
            )
            
            analysis = await self.llm_caller.analyze_users(
                analysis_prompt,
                "åŸºäºç”¨æˆ·å†…å®¹æ•°æ®ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Š"
            )
            
            return analysis if analysis else "æ— æ³•ç”Ÿæˆå†…å®¹åˆ†ææ€»ç»“"
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå†…å®¹åˆ†ææ€»ç»“å¤±è´¥: {e}")
            return f"å†…å®¹åˆ†ææ€»ç»“ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    async def _generate_retrieval_summary(
        self,
        semantic_insights: Dict[str, Any],
        content_analysis: Dict[str, Any], 
        basic_result: AnalysisResult
    ) -> str:
        """ç”Ÿæˆæ™ºèƒ½æ£€ç´¢æ‘˜è¦"""
        
        try:
            summary_parts = []
            
            # åŸºç¡€åˆ†ææ‘˜è¦
            summary_parts.append(f"åŸºç¡€åˆ†æ: è¯†åˆ« {len(basic_result.high_value_users)} ä¸ªé«˜ä»·å€¼ç”¨æˆ·")
            
            # è¯­ä¹‰æ£€ç´¢æ‘˜è¦
            if semantic_insights.get("search_results"):
                search_count = len(semantic_insights["search_results"])
                summary_parts.append(f"è¯­ä¹‰æ£€ç´¢: æ‰§è¡Œ {search_count} ä¸ªæŸ¥è¯¢")
            
            # å†…å®¹åˆ†ææ‘˜è¦
            if content_analysis.get("analyzed_users_count"):
                analyzed_count = content_analysis["analyzed_users_count"]
                summary_parts.append(f"å†…å®¹åˆ†æ: æ·±åº¦åˆ†æ {analyzed_count} ä¸ªç”¨æˆ·")
            
            # æ™ºèƒ½æ´å¯Ÿ
            if semantic_insights.get("insights_summary"):
                summary_parts.append("ç”Ÿæˆæ™ºèƒ½æ´å¯Ÿæ‘˜è¦")
            
            return " | ".join(summary_parts)
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ£€ç´¢æ‘˜è¦å¤±è´¥: {e}")
            return f"æ£€ç´¢æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    async def _enhance_user_profiles(
        self,
        db_session: AsyncSession,
        basic_users: List[UserProfile]
    ) -> List[EnhancedUserProfile]:
        """å¢å¼ºç”¨æˆ·ç”»åƒ"""
        
        enhanced_users = []
        
        for user in basic_users:
            try:
                # è·å–ç”¨æˆ·çš„è¯­ä¹‰æœç´¢ç»“æœ
                user_query = f"ç”¨æˆ· {user.nickname} {user.user_id}"
                search_results = await self.llamaindex_manager.semantic_search(
                    query=user_query,
                    index_type="all",
                    top_k=3,
                    similarity_threshold=0.5
                )
                
                # ç”Ÿæˆç›¸å…³å†…å®¹æ‘˜è¦
                related_summary = self._generate_related_content_summary(search_results)
                
                # ç”ŸæˆAIæ´å¯Ÿ
                ai_insights = await self._generate_user_ai_insights(user, search_results)
                
                # è®¡ç®—æ£€ç´¢åˆ†æ•°
                retrieval_score = self._calculate_retrieval_score(search_results)
                
                # åˆ›å»ºå¢å¼ºç‰ˆç”¨æˆ·ç”»åƒ
                enhanced_user = EnhancedUserProfile(
                    user_id=user.user_id,
                    nickname=user.nickname,
                    emotional_preference=user.emotional_preference,
                    aips_preference=user.aips_preference,
                    has_visited=user.has_visited,
                    unmet_preference=user.unmet_preference,
                    unmet_desc=user.unmet_desc,
                    gender=user.gender,
                    age=user.age,
                    value_score=user.value_score,
                    interaction_count=user.interaction_count,
                    latest_activity=user.latest_activity,
                    notes_engaged=user.notes_engaged,
                    semantic_search_results=search_results,
                    related_content_summary=related_summary,
                    ai_insights=ai_insights,
                    retrieval_score=retrieval_score
                )
                
                enhanced_users.append(enhanced_user)
                
            except Exception as e:
                logger.error(f"âŒ å¢å¼ºç”¨æˆ· {user.user_id} ç”»åƒå¤±è´¥: {e}")
                # å¦‚æœå¢å¼ºå¤±è´¥ï¼Œè‡³å°‘ä¿ç•™åŸºç¡€ä¿¡æ¯
                enhanced_user = EnhancedUserProfile(
                    user_id=user.user_id,
                    nickname=user.nickname,
                    emotional_preference=user.emotional_preference,
                    aips_preference=user.aips_preference,
                    has_visited=user.has_visited,
                    unmet_preference=user.unmet_preference,
                    unmet_desc=user.unmet_desc,
                    gender=user.gender,
                    age=user.age,
                    value_score=user.value_score,
                    interaction_count=user.interaction_count,
                    latest_activity=user.latest_activity,
                    notes_engaged=user.notes_engaged,
                    semantic_search_results=[],
                    related_content_summary="å¢å¼ºå¤±è´¥",
                    ai_insights=f"å¢å¼ºå¤±è´¥: {str(e)}",
                    retrieval_score=0.0
                )
                enhanced_users.append(enhanced_user)
        
        return enhanced_users
    
    def _generate_related_content_summary(self, search_results: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆç›¸å…³å†…å®¹æ‘˜è¦"""
        
        if not search_results:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹"
        
        summary_parts = []
        for result in search_results[:3]:
            content_type = result.get("index_type", "unknown")
            score = result.get("score", 0)
            summary_parts.append(f"{content_type}(ç›¸ä¼¼åº¦:{score:.2f})")
        
        return " | ".join(summary_parts)
    
    async def _generate_user_ai_insights(
        self, 
        user: UserProfile, 
        search_results: List[Dict[str, Any]]
    ) -> str:
        """ç”Ÿæˆç”¨æˆ·AIæ´å¯Ÿ"""
        
        try:
            if not search_results:
                return "æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”ŸæˆAIæ´å¯Ÿ"
            
            # æ•´ç†æœç´¢ç»“æœ
            context_data = []
            for result in search_results[:2]:
                context_data.append(result.get("content", "")[:300])
            
            context = "\n".join(context_data)
            
            # æ„å»ºç”¨æˆ·åŸºç¡€ä¿¡æ¯
            user_info = f"""
ç”¨æˆ·ID: {user.user_id}
æ˜µç§°: {user.nickname}  
ä»·å€¼è¯„åˆ†: {user.value_score}
æƒ…æ„Ÿå€¾å‘: {user.emotional_preference}
äº’åŠ¨æ¬¡æ•°: {user.interaction_count}
æœªæ»¡è¶³éœ€æ±‚: {user.unmet_desc[:100]}...
"""
            
            # ä½¿ç”¨ä¸“ä¸šæç¤ºè¯ç”Ÿæˆæ´å¯Ÿ
            prompt_template = get_user_analyst_prompt("deep_user_analysis")
            
            analysis_prompt = prompt_template.format(
                user_basic_info=user_info,
                user_behavior_data=context,
                comment_sentiment_data="åŸºäºè¯­ä¹‰æœç´¢çš„ç”¨æˆ·è¡Œä¸ºåˆ†æ"
            )
            
            insights = await self.llm_caller.analyze_users(
                analysis_prompt,
                "ç”Ÿæˆå•ä¸ªç”¨æˆ·çš„æ·±åº¦AIæ´å¯Ÿåˆ†æ"
            )
            
            return insights[:500] if insights else "æ— æ³•ç”ŸæˆAIæ´å¯Ÿ"
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆç”¨æˆ·AIæ´å¯Ÿå¤±è´¥: {e}")
            return f"AIæ´å¯Ÿç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def _calculate_retrieval_score(self, search_results: List[Dict[str, Any]]) -> float:
        """è®¡ç®—æ£€ç´¢è¯„åˆ†"""
        
        if not search_results:
            return 0.0
        
        # åŸºäºæœç´¢ç»“æœçš„æ•°é‡å’Œè´¨é‡è®¡ç®—åˆ†æ•°
        total_score = sum(result.get("score", 0) for result in search_results)
        result_count = len(search_results)
        
        # å½’ä¸€åŒ–åˆ†æ•°
        avg_score = total_score / result_count if result_count > 0 else 0
        count_bonus = min(result_count / 5.0, 1.0)  # æœ€å¤š5ä¸ªç»“æœçš„å¥–åŠ±
        
        final_score = (avg_score * 0.7) + (count_bonus * 0.3)
        
        return round(final_score, 3)
    
    async def smart_user_query(self, question: str) -> str:
        """æ™ºèƒ½ç”¨æˆ·æŸ¥è¯¢ - ç›´æ¥å›ç­”å…³äºç”¨æˆ·çš„é—®é¢˜"""
        
        logger.info(f"ğŸ¤” æ™ºèƒ½ç”¨æˆ·æŸ¥è¯¢: {question}")
        
        try:
            # ä½¿ç”¨LlamaIndexè¿›è¡Œæ™ºèƒ½é—®ç­”
            answer = await self.llamaindex_manager.intelligent_query(
                question=question,
                context_type="all",
                max_context_length=2000
            )
            
            return answer if answer else "æŠ±æ­‰ï¼Œæ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
            
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½ç”¨æˆ·æŸ¥è¯¢å¤±è´¥: {e}")
            return f"æŸ¥è¯¢å¤±è´¥: {str(e)}"